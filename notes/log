1) SVM
	All one class-didn't pursue much further
2) Random forest
	Gini/Entropy doesn't do much
	Class_weight matters a bit
	OOB error?
	Overfits
3) Gradient boosted trees
	Slightly worse performance than RF
	Overfits
4) More data
	Still doesn't do well
	xgb now has slightly best performance-does not overfit much on training
	xgb regularization works a bit
5) PCA
	does nothing (use for visualization?)
6) Preprocessing
	Normalize
7) NN
	Simple MLP
	Add Dropout improves performance
	High batch size improves performance?-stops guessing plat/diamond at all
	Only works with ReLU
	Weight init change does not help
	Marginal improvement going from 5->3 class classification

Data to show:
	Classification summary
	Performance of RF as estimators increased
	Comparison of Train/Test accuracy
	Comparison of best estimators
	How models perform as more data is used
	How models perform as they get bigger
Talk about:
	Grid searched parameters

